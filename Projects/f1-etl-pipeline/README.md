# ğŸï¸ F1 ETL Pipeline: API â†’ PostgreSQL â†’ Google Sheets â†’ Looker Studio

This project builds a full end-to-end ETL pipeline using public Formula 1 racing data. It extracts data from the Open F1 API, processes it using Python and pandas, loads it into a PostgreSQL database (hosted locally or on Amazon RDS), and visualises the final results through Looker Studio.

---

## ğŸš€ What pipeline does

1. Extracts race, driver, and weather data from a public API
2. Cleans and transforms the data using Python (`pandas`)
3. Loads the data into a PostgreSQL database
4. Optionally pushes the dataset to Google Sheets
5. Visualises the results using Looker Studio

---

## ğŸ› ï¸ Tech Stack

- **Python 3.11+**
- **pandas** â€“ for data processing
- **psycopg2** â€“ to connect to PostgreSQL
- **gspread & oauth2client** â€“ for Google Sheets integration
- **python-dotenv** â€“ for managing secrets
- **PostgreSQL (AWS RDS)**
- **Looker Studio** â€“ for data visualisation

---

## ğŸ“ Project Structure
```
f1-etl-project/
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract_api_data.py
â”‚ â”œâ”€â”€ load_to_postgres.py
â”‚ â”œâ”€â”€ transform_data.py
â”‚ â””â”€â”€ load_to_gsheet.py
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ xxx.json (ğŸ” Google credentials â€“ not in repo)
â”‚
â”œâ”€â”€ .env â† Contains database and API paths (not tracked)
â”œâ”€â”€ .env.example â† Template for your environment variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/n-chistyakov/git-repo/tree/main/Projects/f1-etl-pipeline
cd f1-etl-pipeline
```

### 2. Create and activate virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

```bash
cp .env.example .env
```

Then update `.env` with your database details and the path to your Google API key (if using Google Sheets).

---

## â–¶ï¸ Running the Pipeline

You can run each script manually:

```bash
python scripts/extract_api_data.py
python scripts/load_to_postgres.py
python scripts/transform_data.py
python scripts/load_to_gsheet.py
```

---

## ğŸ“Š Visual Output

* Data visualised in **Looker Studio** via Google Sheets connection
* Sample dashboard: [Looker](https://lookerstudio.google.com/s/qUlGVQlW5ao)

---

## ğŸ”’ Security Notes

* `.env` and Google credentials are **excluded** from the repo via `.gitignore`
* Always create your own Google service account key if you wish to run the Google Sheets upload

---

## ğŸ™‡â€â™‚ï¸ Contact

Built by **\[Nikita Chistyakov]**
[LinkedIn](https://www.linkedin.com/in/nikitachistyakov) | [GitHub](https://github.com/n-chistyakov/git-repo)




